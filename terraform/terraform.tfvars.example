# K3s Multi-Cloud Cluster Configuration Example
# Copy this file to terraform.tfvars and fill in your values

###############################################################################
# Global Configuration
###############################################################################

cluster_name = "k3s-multicloud"
k3s_token    = "your-secure-random-token-here"  # Generate with: openssl rand -base64 48
ssh_key_path = "~/.ssh/id_rsa.pub"

###############################################################################
# Cloud Provider Toggles
###############################################################################

enable_openstack = true
enable_aws       = false  # Future support
enable_gcp       = false  # Future support

###############################################################################
# OpenStack Configuration
###############################################################################

user_name              = "your-openstack-username"
user_password          = "your-openstack-password"
tenant_name            = "your-openstack-project"
openstack_auth_url     = "https://your-openstack-endpoint:5000/v3"
openstack_region       = "RegionOne"
openstack_cacert_file  = "./os-trusted-cas"

# Cluster sizing
openstack_server_count = 3  # Control plane servers (1-5)
openstack_agent_count  = 3  # Worker nodes

# Instance flavors
openstack_server_flavor  = "m1.medium"
openstack_agent_flavor   = "m1.medium"
openstack_bastion_flavor = "m1.small"

# Network configuration
openstack_network_cidr      = "192.168.255.0/24"
openstack_dns_servers       = ["10.33.16.100", "8.8.8.8"]
openstack_floating_ip_pool  = "ext_net"

###############################################################################
# Feature Flags
###############################################################################

enable_bastion       = true   # Set to false when using Tailscale for SSH
enable_load_balancer = true   # Keep true for HA control plane

###############################################################################
# Tailscale VPN Configuration (Optional)
###############################################################################

enable_tailscale = false  # Set to true to enable Tailscale mesh VPN with ephemeral keys

# Tailscale API Key (OAuth Client Secret)
# Create at: https://login.tailscale.com/admin/settings/oauth
# Required scopes: devices:write, auth_keys:write
# The provider will auto-generate ephemeral auth keys per node
tailscale_api_key = ""  # Example: "tskey-api-xxxxxxxxxxxxx-yyyyyyyyyyyyyyyyyy"

# Your Tailscale tailnet identifier
# Examples: "user@example.com" or "example.com"
# Find at: Tailscale Admin Console → Settings → General → Tailnet name
tailscale_tailnet = ""  # Example: "user@example.com"

# Optional: Custom hostname prefix for MagicDNS (defaults to cluster_name)
tailscale_hostname_prefix = ""  # Example: "k3s-prod"

# Optional: Auth key expiry (seconds, default: 7200 = 2 hours)
# Increase for slow deployments
tailscale_key_expiry = 7200

# Optional: IP update check interval (seconds, default: 300 = 5 minutes)
# How often to check for Tailscale IP changes on ephemeral reconnects
tailscale_ip_update_interval = 300

###############################################################################
# Tailscale OAuth Credentials for In-Cluster Helm Chart (Optional)
###############################################################################

# Tailscale OAuth Client ID
# Create a separate OAuth application in Tailscale Admin Console for in-cluster use
# This is independent of the tailscale_api_key used for ephemeral node keys
# Leave empty if not using Tailscale Helm chart with OAuth
tailscale_oauth_client_id = ""

# Tailscale OAuth Client Secret
# Pair with the client ID above for in-cluster Tailscale Helm chart
# This will be stored in a Kubernetes Secret in the tailscale namespace
tailscale_oauth_client_secret = ""

###############################################################################
# Cloudflare Tunnel Configuration (Optional)
###############################################################################

# Enable Cloudflare Tunnel for public internet access to cluster services
enable_cloudflare_tunnel = false

# Cloudflare Account ID (not sensitive, can be committed)
# Find this in your Cloudflare dashboard URL: https://one.dash.cloudflare.com/<account-id>/
cloudflare_account_id = ""  # Example: "bb5031d201f956c338ae9bb052ddfee1"

# Cloudflare Tunnel ID (not sensitive, can be committed)
# Create a tunnel at: https://one.dash.cloudflare.com/ > Networks > Tunnels
# The tunnel ID is a UUID shown in the tunnel details
cloudflare_tunnel_id = ""  # Example: "4c2ab3f8-ac2b-4a70-a8e1-3a00397bdee2"

# Cloudflare Tunnel Secret (SENSITIVE - do not commit!)
# This is found in the tunnel credentials JSON or the connector installation command
# Keep this in your local terraform.tfvars file only (gitignored)
cloudflare_tunnel_secret = ""

###############################################################################
# Security Configuration
###############################################################################

# TODO: Restrict these CIDR ranges for production deployments
external_ssh_cidrs = ["0.0.0.0/0"]  # Allowed IPs for SSH access
external_api_cidrs = ["0.0.0.0/0"]  # Allowed IPs for K8s API access

###############################################################################
# GPU Support Configuration
###############################################################################
# Enable NVIDIA GPU Operator for automatic GPU support
# This will automatically deploy all GPU components including:
# - Node Feature Discovery
# - GPU Feature Discovery (node labeling)
# - NVIDIA Container Toolkit
# - NVIDIA Device Plugin
# - NVIDIA DCGM Exporter (metrics)
enable_nvidia_gpu_operator = false

###############################################################################
# Longhorn Distributed Storage Configuration
###############################################################################
# Enable Longhorn for ReadWriteMany volume support (multi-pod access)
# Longhorn provides distributed block storage with replication across nodes
enable_longhorn = true

# Size of dedicated Cinder volume attached to each agent for Longhorn storage (in GiB)
# Total storage = longhorn_storage_size * agent_count
# Example: 50 GiB * 3 agents = 150 GiB raw storage
longhorn_storage_size = 50

# Number of replicas for Longhorn volumes (recommended: match agent_count for full HA)
# 3 replicas = data replicated across 3 nodes for high availability
longhorn_replica_count = 3

# Enable S3-compatible backup target using OpenStack Swift (enabled by default)
# This creates a Swift container for automated Longhorn backups and disaster recovery
# The Swift container is protected from deletion when cluster is destroyed
enable_longhorn_backup = true

# S3 endpoint URL for OpenStack Swift (leave empty to auto-detect)
# Auto-detection replaces port 5000 with 8080 in the auth_url
# Example: "https://swift.example.com:8080"
longhorn_backup_s3_endpoint = ""

# S3 region for backups (typically matches OpenStack region)
longhorn_backup_s3_region = "RegionOne"

###############################################################################
# ArgoCD GitOps Configuration
###############################################################################
# Enable ArgoCD for GitOps continuous deployment
enable_argocd = true

# ArgoCD admin password (optional)
# Leave empty to auto-generate. Get password with:
# kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath="{.data.password}" | base64 -d
# Or set a bcrypt-hashed password. Generate with:
# htpasswd -nbBC 10 "" 'yourpassword' | tr -d ':\n' | sed 's/$2y/$2a/'
argocd_admin_password = ""

# Git repository URL for ArgoCD applications
# - GitHub: https://github.com/username/repository.git
argocd_repo_url = ""


